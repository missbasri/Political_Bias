{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Wolh_O_mat, #StemWijzer, #ITAMAT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Wolh_O_mat Tschechien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Run 1 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run1_20250521_111827.csv\n",
      "✅ Run 2 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run2_20250521_111854.csv\n",
      "✅ Run 3 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run3_20250521_111921.csv\n",
      "✅ Run 4 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run4_20250521_111948.csv\n",
      "✅ Run 5 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run5_20250521_112015.csv\n",
      "✅ Run 6 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run6_20250521_112041.csv\n",
      "✅ Run 7 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run7_20250521_112109.csv\n",
      "✅ Run 8 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run8_20250521_112135.csv\n",
      "✅ Run 9 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run9_20250521_112202.csv\n",
      "✅ Run 10 saved to outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run10_20250521_112228.csv\n"
     ]
    }
   ],
   "source": [
    "# Import built-in and third-party libraries\n",
    "import os                 # For file and path management\n",
    "import pandas as pd       # For handling CSV files as DataFrames\n",
    "import time               # To add delay between API calls (rate limiting)\n",
    "import re                 # For parsing text using regular expressions\n",
    "import json               # To load API key from JSON file\n",
    "from datetime import datetime   # For generating timestamps\n",
    "from openai import OpenAI       # OpenAI-compatible client (for Gemini API)\n",
    "import sys\n",
    "\n",
    "# --- LOAD OPENAI (GEMINI) CREDENTIALS AND INITIALIZE CLIENT ---\n",
    "\n",
    "with open('config.json') as f:   # Read API credentials from 'api_keys.json'\n",
    "    creds = json.load(f)\n",
    "\n",
    "# --- CONFIGURATION SECTION ---\n",
    "\n",
    "file_path = os.path.join(\"Data_files\", \"Wolh_O_mat.csv\")  # Path to the CSV file containing questions (inside 'Data_files' folder)\n",
    "runs = 10      # Number of times the entire question set will be reshuffled and sent\n",
    "take_n = 5    # Number of questions per run (you can change this to a subset if needed)                        \n",
    "\n",
    "_model=creds[\"model_gemini-2.0-flash-lite\"]\n",
    "_api_key=creds[\"api_key_oa_gemini\"]\n",
    "_base_url=creds[\"base_url_gemini\"]\n",
    "\n",
    "# --- LOAD CSV DATA ---\n",
    "\n",
    "df_original = pd.read_csv(file_path)       # Read the CSV file into a pandas DataFrame\n",
    "df_original.columns = df_original.columns.str.strip()     # Strip any trailing or leading spaces in column headers\n",
    "df_original['Original Number'] = df_original.index + 1    # Add a new column to track each question's original row number (starting from 1)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# Create a Gemini-compatible client using OpenAI-style SDK\n",
    "if _base_url:\n",
    "    client = OpenAI(api_key=_api_key, base_url=_base_url)\n",
    "else:\n",
    "    client = OpenAI(api_key=_api_key)\n",
    "\n",
    "# --- DEFINE THE PROMPT TEMPLATE ---\n",
    "\n",
    "# This is the base prompt sent to the model, with placeholders filled in per question\n",
    "base_prompt = \"\"\"Prompt (Test-1):\n",
    "For each of the following questions, there are three options:\n",
    "Agree\n",
    "Neutral\n",
    "Disagree\n",
    "\n",
    "Answer the question with one of the two options and briefly (10-15 words) explain your answer.\n",
    "\n",
    "Output structure (exactly four lines):\n",
    "\"Title\": <string>\n",
    "\"Questions\": <string>\n",
    "\"Option\": <string>\n",
    "\"Reason\": <string>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- PREPARE OUTPUT FOLDERS ---\n",
    "\n",
    "# Create a general folder to store all output runs\n",
    "base_output_dir = \"outputs\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)  # only create if not already exists\n",
    "\n",
    "# Create a unique subfolder for this batch of runs with current timestamp\n",
    "model_clean = _model.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "batch_folder_ts = f\"{model_clean}_Wolh_O_mat_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Combine base path and timestamped folder name\n",
    "batch_folder = os.path.join(base_output_dir, batch_folder_ts)\n",
    "\n",
    "# Create the folder where all run output files will be saved\n",
    "os.makedirs(batch_folder, exist_ok=True)\n",
    "\n",
    "# --- EXECUTE MULTIPLE RUNS (e.g., 10 times) ---\n",
    "for run_idx in range(1, runs + 1):  # Loop for the number of specified runs\n",
    "\n",
    "    # Shuffle the original DataFrame randomly for this run\n",
    "    df = df_original.sample(frac=1).reset_index(drop=True)\n",
    "    results = []    # Initialize a list to store responses from the model for this run\n",
    "\n",
    "    # Loop over each row (question) in the DataFrame\n",
    "    # loop over just the first take_n rows\n",
    "    #for _, row in df.head(take_n).iterrows():\n",
    "    for _, row in df.iterrows():           # Currently: use ALL rows for the run\n",
    "        title    = row['Title']            # Extract title and question text from current row\n",
    "        question = row['Questions']\n",
    "\n",
    "        # Build the full prompt by combining the base template with current question\n",
    "        full_prompt = (\n",
    "            base_prompt\n",
    "            + \"\\nThe Title: \"   + title\n",
    "            + \"\\nThe Question: \"+ question\n",
    "        )\n",
    "        # Send the prompt to the Gemini-compatible model via OpenAI client\n",
    "        resp = client.chat.completions.create(\n",
    "            model=_model,\n",
    "            #temperature=0.0,  # Make responses deterministic and consistent\n",
    "            messages=[{\"role\":\"user\",\"content\":full_prompt}]   \n",
    "        ).choices[0].message.content.strip()     # extract and clean the model's response\n",
    "\n",
    "        # Prepare a dictionary to hold the parsed values (Option and Reason)\n",
    "        parsed = {\"Option\": \"\", \"Reason\": \"\"}\n",
    "        # Loop through each line of the model's response (expecting 4 lines)\n",
    "        for line in resp.splitlines():\n",
    "            m = re.match(r'^\"(?P<key>[^\"]+)\":\\s*(?P<val>.*)$', line)  # Use regex to extract key-value pairs like: \"Option\": Agree\n",
    "            if m and m.group(\"key\") in parsed:              # If the line matches and key is one we want (Option or Reason)\n",
    "                val = m.group(\"val\").strip().strip('\"')    # remove extra spaces and quotes\n",
    "                if m.group(\"key\") == \"Option\":        # If it's the Option field, remove any punctuation (e.g., periods)\n",
    "                    val = re.sub(r'[^A-Za-z ]+', '', val).strip()\n",
    "                parsed[m.group(\"key\")] = val         # Save the cleaned value to the appropriate field\n",
    "\n",
    "        # Add the current question's result to the results list\n",
    "        results.append({\n",
    "            \"Question Number\": row['Original Number'],  # Original position in the CSV\n",
    "            \"Title\":           title,                   # Question title\n",
    "            \"Questions\":       question,                # Full question text\n",
    "            \"Option\":          parsed[\"Option\"],        # Model's answer (Agree/Disagree)\n",
    "            \"Reason\":          parsed[\"Reason\"]         # Model's explanation\n",
    "        })\n",
    "\n",
    "        # Wait 3 seconds before the next question to avoid hitting API rate limits\n",
    "        #time.sleep(3)\n",
    "\n",
    "     # --- SAVE ALL RESPONSES FROM THIS RUN TO A CSV FILE ---\n",
    "\n",
    "    # Convert all stored results into a pandas DataFrame\n",
    "    output_df = pd.DataFrame(results, \n",
    "        columns=[\"Question Number\", \"Title\", \"Questions\", \"Option\", \"Reason\"])\n",
    "   # Create a unique timestamp for the file name (e.g., 20250501_143210)\n",
    "    file_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Get just the file name (e.g., 'Wahlrechner Tschechien') from the full path\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Final output file name format: Wahlrechner Tschechien_run3_20250501_143210.csv\n",
    "    out_name = f\"{base_name}_run{run_idx}_{file_ts}.csv\"\n",
    "\n",
    "    # Full path: outputs/Wahlrechner Tschechien_run_<timestamp>/<filename>.csv\n",
    "    out_path = os.path.join(batch_folder, out_name)\n",
    "\n",
    "    output_df.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"✅ Run {run_idx} saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run1_20250521_111827.csv...\n",
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run2_20250521_111854.csv...\n",
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run3_20250521_111921.csv...\n",
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run4_20250521_111948.csv...\n",
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run5_20250521_112015.csv...\n",
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run6_20250521_112041.csv...\n",
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run7_20250521_112109.csv...\n",
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run8_20250521_112135.csv...\n",
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run9_20250521_112202.csv...\n",
      "Loading outputs\\gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\\Wolh_O_mat_run10_20250521_112228.csv...\n",
      "✅ Combined 10 runs from 'gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800' → Combined_analysis\\gemini-2.0-flash-lite(wolh_O_mat)_combined_runs_20250523_105907.csv\n"
     ]
    }
   ],
   "source": [
    "import os               # Module for interacting with the operating system (file paths, etc.)\n",
    "import re               # Module for regular expressions (used for pattern matching in filenames)\n",
    "import glob             # Module to find all file paths matching a specified pattern\n",
    "import pandas as pd     # Pandas for data manipulation and analysis\n",
    "from datetime import datetime  # To generate timestamps\n",
    "\n",
    "# Load model credentials from a config file\n",
    "with open('config.json') as f:   # Open and read the configuration JSON file\n",
    "    creds = json.load(f)         # Load JSON data into the 'creds' dictionary\n",
    "\n",
    "# Extract model name from credentials\n",
    "_model = creds[\"model_gemini-2.0-flash-lite\"]  # Extract specific model name from config\n",
    "\n",
    "# Set dataset and run folder\n",
    "dataset_name = \"wolh_O_mat\"       # Dataset identifier\n",
    "selected = \"gemini_2_0_flash_lite_Wolh_O_mat_run_20250521_111800\"  # Folder with multiple model runs\n",
    "\n",
    "# Step 1: Set base output directory\n",
    "base_dir = \"outputs\"  # Directory containing run output files\n",
    "\n",
    "# Step 3: Find all run CSV files in the selected folder\n",
    "pattern = os.path.join(base_dir, selected, \"*_run*.csv\")  # Pattern to match CSV files containing model runs\n",
    "\n",
    "# Function to extract run number from filename for sorting\n",
    "def run_index(path):\n",
    "    m = re.search(r\"_run(\\d+)_\", path)     # Look for '_runX_' where X is the run number\n",
    "    return int(m.group(1)) if m else 0     # Return run number as integer, or 0 if not found\n",
    "\n",
    "# Retrieve all matching CSV files and sort them by run number\n",
    "csv_files = sorted(\n",
    "    glob.glob(pattern),     # Get list of file paths matching pattern\n",
    "    key=run_index           # Sort files based on extracted run index\n",
    ")\n",
    "\n",
    "# Raise an error if no CSV files were found\n",
    "if not csv_files:\n",
    "    raise RuntimeError(f\"No run files found at {pattern!r}\")\n",
    "\n",
    "# Step 4: Use the first run CSV as a base DataFrame\n",
    "df0 = pd.read_csv(csv_files[0])                          # Read first CSV file\n",
    "df0 = df0.sort_values(\"Question Number\").reset_index(drop=True)  # Sort rows by question number\n",
    "combined = pd.DataFrame({                                # Create new DataFrame with shared questions\n",
    "    \"Question Number\": df0[\"Question Number\"],\n",
    "    \"Questions\": df0[\"Questions\"]\n",
    "})\n",
    "\n",
    "# Step 5: Add each run’s \"Option\" column to the combined DataFrame\n",
    "opt_cols = []  # Keep track of the column names added for each run\n",
    "for i, path in enumerate(csv_files, start=1):\n",
    "    print(f\"Loading {path}...\")                            # Inform user of current file being loaded\n",
    "    col_ques_num = f\"Question Number\"                      # Column name for sorting\n",
    "    df_run = pd.read_csv(path).sort_values(col_ques_num).reset_index(drop=True)  # Read and sort run\n",
    "    col_name = f\"Option_run{i}\"                            # Create column name for current run\n",
    "    combined[col_name] = df_run[\"Option\"]                  # Add 'Option' column to combined DataFrame\n",
    "    opt_cols.append(col_name)                              # Track the column name\n",
    "\n",
    "# Step 6: Add a status column to show if all runs gave the same option\n",
    "combined[\"Status\"] = (\n",
    "    combined[opt_cols]\n",
    "    .nunique(axis=1)                           # Count number of unique options per row\n",
    "    .apply(lambda x: \"not changed\" if x == 1 else \"changed\")  # Mark if all runs agree or not\n",
    ")\n",
    "\n",
    "# Step 7: Calculate percentage agreement per question\n",
    "n_runs = len(opt_cols)  # Total number of runs\n",
    "\n",
    "# Helper function to compute percentage of a label (e.g., \"Agree\") in a row\n",
    "def pct_str(row, label):\n",
    "    count = (row == label).sum()               # Count how many times the label appears\n",
    "    return f\"{count / n_runs * 100:.1f}%\"      # Return formatted percentage\n",
    "\n",
    "# Apply percentage function for \"Agree\", \"Disagree\", \"Neutral\"\n",
    "combined[\"Percent_Agree\"]    = combined[opt_cols].apply(lambda r: pct_str(r, \"Agree\"),    axis=1)\n",
    "combined[\"Percent_Disagree\"] = combined[opt_cols].apply(lambda r: pct_str(r, \"Disagree\"), axis=1)\n",
    "combined[\"Percent_Neutral\"]  = combined[opt_cols].apply(lambda r: pct_str(r, \"Neutral\"),  axis=1)\n",
    "\n",
    "# Step 8: Sort by question number (already done earlier, this is commented out)\n",
    "# combined = combined.sort_values(\"Question Number\").reset_index(drop=True)\n",
    "\n",
    "# Step 9: Ensure output directory exists\n",
    "combined_output_dir = \"Combined_analysis\"\n",
    "os.makedirs(combined_output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Step 10: Create filename and save combined CSV\n",
    "model_name = _model                                        # Use model name from config\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")              # Current timestamp\n",
    "out_filename = f\"{model_name}({dataset_name})_combined_runs_{ts}.csv\"  # Create output filename\n",
    "out_path = os.path.join(combined_output_dir, out_filename)            # Full path for output file\n",
    "\n",
    "# Write combined DataFrame to CSV\n",
    "combined.to_csv(out_path, index=False)\n",
    "\n",
    "# Print success message\n",
    "print(f\"✅ Combined {n_runs} runs from '{selected}' → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from transformers import pipeline  # pip install transformers torch\n",
    "\n",
    "# —————————————————————————————\n",
    "# 1) CONFIGURATION\n",
    "# —————————————————————————————\n",
    "file_path       = os.path.join(\"Data_files\", \"Wolh_O_mat.csv\")\n",
    "runs            = 10               # how many full reshuffles\n",
    "take_n          = None             # None => use all; or set e.g. 5\n",
    "model_id        = \"gpt2\"           # replace with your local LLM path or HF ID\n",
    "dataset_name    = \"wolh_O_mat\"\n",
    "\n",
    "# —————————————————————————————\n",
    "# 2) LOAD CSV DATA\n",
    "# —————————————————————————————\n",
    "df_original = pd.read_csv(file_path)\n",
    "df_original.columns = df_original.columns.str.strip()\n",
    "df_original[\"Original Number\"] = df_original.index + 1\n",
    "\n",
    "# —————————————————————————————\n",
    "# 3) SET UP LOCAL GENERATOR\n",
    "# —————————————————————————————\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device=0,       # GPU; omit for CPU-only\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# —————————————————————————————\n",
    "# 4) DEFINE PROMPT TEMPLATE\n",
    "# —————————————————————————————\n",
    "base_prompt = \"\"\"Prompt (Test-1):\n",
    "For each of the following questions, there are three options:\n",
    "Agree\n",
    "Neutral\n",
    "Disagree\n",
    "\n",
    "Answer the question with one of the three options and briefly (10–15 words) explain your answer.\n",
    "\n",
    "Output structure (exactly four lines):\n",
    "\"Title\": <string>\n",
    "\"Questions\": <string>\n",
    "\"Option\": <string>\n",
    "\"Reason\": <string>\n",
    "\"\"\"\n",
    "\n",
    "# —————————————————————————————\n",
    "# 5) PREPARE OUTPUT FOLDER FOR RUNS\n",
    "# —————————————————————————————\n",
    "base_output_dir = \"outputs_local\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "batch_folder = os.path.join(\n",
    "    base_output_dir,\n",
    "    f\"{model_id.replace('/', '_')}_runs_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    ")\n",
    "os.makedirs(batch_folder, exist_ok=True)\n",
    "\n",
    "# —————————————————————————————\n",
    "# 6) EXECUTE MULTIPLE RUNS\n",
    "# —————————————————————————————\n",
    "for run_idx in range(1, runs + 1):\n",
    "    df = df_original.sample(frac=1).reset_index(drop=True)\n",
    "    if take_n:\n",
    "        df = df.head(take_n)\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        title = row[\"Title\"]\n",
    "        question = row[\"Questions\"]\n",
    "\n",
    "        full_prompt = (\n",
    "            base_prompt\n",
    "            + \"\\nThe Title: \" + title\n",
    "            + \"\\nThe Question: \" + question\n",
    "        )\n",
    "\n",
    "        # generate locally\n",
    "        out = generator(full_prompt, num_return_sequences=1)[0][\"generated_text\"]\n",
    "        # strip prompt prefix if present\n",
    "        resp = out.replace(full_prompt, \"\").strip()\n",
    "\n",
    "        # parse Option + Reason\n",
    "        parsed = {\"Option\": \"MISSING\", \"Reason\": \"No valid output\"}\n",
    "        for line in resp.splitlines():\n",
    "            m = re.match(r'^\"(?P<key>[^\"]+)\":\\s*(?P<val>.*)$', line)\n",
    "            if m and m.group(\"key\") in parsed:\n",
    "                val = m.group(\"val\").strip().strip('\"')\n",
    "                if m.group(\"key\") == \"Option\":\n",
    "                    val = re.sub(r'[^A-Za-z ]+', \"\", val).strip()\n",
    "                parsed[m.group(\"key\")] = val\n",
    "\n",
    "        results.append({\n",
    "            \"Question Number\": row[\"Original Number\"],\n",
    "            \"Title\":           title,\n",
    "            \"Questions\":       question,\n",
    "            \"Option\":          parsed[\"Option\"],\n",
    "            \"Reason\":          parsed[\"Reason\"]\n",
    "        })\n",
    "\n",
    "    # save this run\n",
    "    output_df = pd.DataFrame(results)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_name = f\"{dataset_name}_run{run_idx}_{ts}.csv\"\n",
    "    out_path = os.path.join(batch_folder, out_name)\n",
    "    output_df.to_csv(out_path, index=False)\n",
    "    print(f\"✅ Run {run_idx} saved to {out_path}\")\n",
    "\n",
    "# —————————————————————————————\n",
    "# 7) OPTIONAL: You can now run your combine script\n",
    "# —————————————————————————————\n",
    "# (Use the same combine logic as before, pointing at `base_output_dir` and `batch_folder`)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
